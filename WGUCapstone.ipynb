{"cells":[{"cell_type":"markdown","metadata":{"id":"yKepqgl6Klqk"},"source":["## Install and Import needed packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"68hcMyWnWLfP"},"outputs":[],"source":["!pip install anvil-uplink\n","!pip install tensorflow-text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zIeWukHGGzmN"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","import string\n","import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","import tensorflow_hub as hub\n","import tensorflow as tf\n","from tensorflow.keras.layers import TextVectorization\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import tensorflow_text as tf_text\n","\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"1SXZryHsIfdo"},"source":["## Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yAt0_4jBORzN"},"outputs":[],"source":["def plot_graphs(history, metric):\n","  plt.plot(history.history[metric])\n","  plt.plot(history.history['val_'+metric], '')\n","  plt.xlabel(\"Epochs\")\n","  plt.ylabel(metric)\n","  plt.legend([metric, 'val_'+metric])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d2P_M_s_Nkhy"},"outputs":[],"source":[" def get_data():\n","    data_path = \"./labeled_data.csv.zip\"\n","    df = pd.read_csv(data_path, index_col=0)\n","    df = df.sample(frac=1).reset_index(drop=True)\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y-0K6ptSO0bx"},"outputs":[],"source":["def get_summary(df):\n","\n","    content = df[\"tweet\"].values\n","    word_tok = [word.lower() for item in content for word in nltk.word_tokenize(item)]\n","    st_words = set(word_tok)\n","\n","    fact = {\n","        \"TotalCount\": len(content),\n","        \"TotalWords\": len(word_tok),\n","        \"TotalUniqueWords\": len(st_words),\n","        \"MeanWordsPerTweet\": len(word_tok) / len(content),\n","    }\n","\n","    return fact, df.describe()"]},{"cell_type":"markdown","metadata":{"id":"imInxZeEK6kW"},"source":["## Load the Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dx5hLfk3NoqY"},"outputs":[],"source":["raw_tweets = get_data()\n","raw_tweets.head()"]},{"cell_type":"markdown","metadata":{"id":"fdnOkeH0LM5P"},"source":["## Expolore the Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i_Zce7LCDVDy"},"outputs":[],"source":["f, s = get_summary(raw_tweets)\n","print(f)\n","print(s)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3r53tFtJOEug"},"outputs":[],"source":["ax = raw_tweets.groupby('class').count().plot(\n","    kind='bar',\n","    title='Distribution of data',\n","    legend=True\n",").set_xticklabels([\n","    'Hate Speech',\n","    'Offensive Language',\n","    'Neither'\n","], rotation=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LPbSVo-XJxSv"},"outputs":[],"source":["raw_tweets.loc[raw_tweets['neither'] > 0, 'class'] = 2\n","raw_tweets.loc[raw_tweets['offensive_language'] > 0, 'class'] = 1\n","raw_tweets.loc[raw_tweets['hate_speech'] > 0, 'class'] = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hwlAOyX5Fxl1"},"outputs":[],"source":["raw_tweets = raw_tweets.drop(['count', 'hate_speech', 'offensive_language', 'neither'], axis=1)\n","raw_tweets.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UFfCMjReKWgc"},"outputs":[],"source":["ax = raw_tweets.groupby('class').count().plot(\n","    kind='bar',\n","    title='Distribution of data',\n","    legend=True\n",").set_xticklabels([\n","    'Hate Speech',\n","    'Offensive Language',\n","    'Neither'\n","], rotation=0)"]},{"cell_type":"markdown","metadata":{"id":"7d3dY-JKL1bM"},"source":["## Prepare the DataSet for training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"67E5wRVj1k6Z"},"outputs":[],"source":["usable_tweets = raw_tweets[raw_tweets['class'] != 1]\n","usable_tweets.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yVonSDTK6c53"},"outputs":[],"source":["mask = usable_tweets['class'] == 2\n","hate = usable_tweets[~mask]\n","neither = usable_tweets[mask]\n","print(neither.shape[0])\n","hate = hate.sample(n=neither.shape[0])\n","print(hate.shape[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WMk08cJP7190"},"outputs":[],"source":["frames = [hate, neither]\n","eqaulized = pd.concat(frames, axis=0)\n","eqaulized = eqaulized.reset_index(drop=True)\n","eqaulized"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_SocQ-_EOdXJ"},"outputs":[],"source":["ax = eqaulized.groupby('class').count().plot(\n","    kind='bar',\n","    title='Distribution of data',\n","    legend=True\n",").set_xticklabels([\n","    'Hate Speech',\n","    'Neither'\n","], rotation=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CZeoYY6kVBJ_"},"outputs":[],"source":["eqaulized.loc[eqaulized['class'] == 0, 'class'] = 1\n","eqaulized.loc[eqaulized['class'] == 2, 'class'] = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lxG1bYxd83Xm"},"outputs":[],"source":["raw_data = usable_tweets\n","train_data = eqaulized.sample(frac = 0.8)\n","test_data = eqaulized.drop(train_data.index)\n","train_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-7hTxf4xKZfl"},"outputs":[],"source":["raw_tweets, raw_sentiment = list(raw_data['tweet']), list(raw_data['class'])\n","test_tweets, test_sentiment = list(test_data['tweet']), list(test_data['class'])\n","train_tweets, train_sentiment = list(train_data['tweet']), list(train_data['class'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CutGFugr6ipP"},"outputs":[],"source":["raw_tweets_ds = tf.convert_to_tensor(raw_tweets)\n","raw_sentiment_ds = tf.convert_to_tensor(raw_sentiment)\n","test_tweets_ds = tf.convert_to_tensor(test_tweets)\n","test_sentiment_ds = tf.convert_to_tensor(test_sentiment)\n","train_tweets_ds = tf.convert_to_tensor(train_tweets)\n","train_sentiment_ds = tf.convert_to_tensor(train_sentiment)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SUchRhjj8VkI"},"outputs":[],"source":["raw_ds = tf.data.Dataset.from_tensors((raw_tweets, raw_sentiment))\n","test_ds = tf.data.Dataset.from_tensors((test_tweets, test_sentiment))\n","train_ds = tf.data.Dataset.from_tensors((train_tweets, train_sentiment))\n","\n","for example, label in train_ds.take(1):\n","  print('text: ', example[0].numpy())\n","  print('label: ', label[0].numpy())\n","train_ds.take(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z9TcCl2N96u3"},"outputs":[],"source":["BUFFER_SIZE = 10000\n","BATCH_SIZE = 64"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZxjJXVAJ9zO8"},"outputs":[],"source":["train_dataset = train_ds.shuffle(BUFFER_SIZE).repeat(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n","test_dataset = test_ds.repeat(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n","train_dataset"]},{"cell_type":"markdown","metadata":{"id":"UPp-V4WZ-dtZ"},"source":["## Create Encoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qx1PZv39-hAS"},"outputs":[],"source":["VOCAB_SIZE = 38016\n","encoder = tf.keras.layers.TextVectorization(\n","    max_tokens=VOCAB_SIZE)\n","encoder.adapt(train_dataset.map(lambda text, label: text))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1rrMndKE-oUh"},"outputs":[],"source":["vocab = np.array(encoder.get_vocabulary())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H2yHpt_j-uoB"},"outputs":[],"source":["encoded_example = encoder(example[:3]).numpy()\n","encoded_example"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w-gUnVBK_L1F"},"outputs":[],"source":["for n in range(3):\n","  print(\"Original: \", example[n].numpy())\n","  print(\"Round-trip: \", \" \".join(vocab[encoded_example[n]]))\n","  print()"]},{"cell_type":"markdown","metadata":{"id":"n7tftfYWoiMN"},"source":["## Building the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EPlSC5xT6ejR"},"outputs":[],"source":["model = tf.keras.Sequential([\n","    encoder,\n","    tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 64, mask_zero=True),\n","    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n","    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n","    tf.keras.layers.Dense(64, activation='relu'),\n","    tf.keras.layers.Dropout(0.5),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JjBeBRU4CVL3"},"outputs":[],"source":["print([layer.supports_masking for layer in model.layers])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hWo6ZbO4Cbft"},"outputs":[],"source":["sample_text = ('RT @SkylarLogsdon: @viva_based bruh you fucked the shit out of my taste buds with that bitch')\n","predictions = model.predict(np.array([sample_text]))\n","print(predictions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lEHj8gLXCi6d"},"outputs":[],"source":["padding = \"the \" * 2000\n","predictions = model.predict(np.array([sample_text, padding]))\n","print(predictions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ngfTJTHICoU0"},"outputs":[],"source":["model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n","              optimizer=tf.keras.optimizers.Adam(1e-4),\n","              metrics=['accuracy'])\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"ullMYML-osR-"},"source":["## Triaining the model"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"PKDPTgpVgJbD"},"outputs":[],"source":["epochs = 2  #@param {type: \"slider\", min: 1, max: 10}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nRt-2R3g6Sx0"},"outputs":[],"source":["history = model.fit(train_dataset, epochs=epochs,\n","                    validation_data=test_dataset, validation_steps=30)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8pMkw_196gb3"},"outputs":[],"source":["test_loss, test_acc = model.evaluate(test_ds)\n","\n","print('Test Loss:', test_loss)\n","print('Test Accuracy:', test_acc)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iWH8rJ6BaPau"},"outputs":[],"source":["def get_string_label(score):\n","    score = score[0][0]\n","    if score < .5:\n","        return \"Neither\"\n","    return \"Hate Speech\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y8q7MnzDEUYD"},"outputs":[],"source":["sample_text = ('RT @SkylarLogsdon: @viva_based bruh you fucked the shit out of my taste buds with that bitch')\n","predictions = model.predict(np.array([sample_text]))\n","print(get_string_label(predictions))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QxxI1aT_OHzD"},"outputs":[],"source":["plt.figure(figsize=(16, 8))\n","plt.subplot(1, 2, 1)\n","plot_graphs(history, 'accuracy')\n","plt.ylim(None, 1)\n","plt.subplot(1, 2, 2)\n","plot_graphs(history, 'loss')\n","plt.ylim(0, None)"]},{"cell_type":"markdown","metadata":{"id":"fhrRP5t7hiYO"},"source":["# User Interaction"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"vgG5A-r8gseX"},"outputs":[],"source":["tweet = \"\\\"I hate people\\\"\"  #@param {type: \"string\"}\n","print(get_string_label(model.predict(np.array([tweet]))))"]}],"metadata":{"colab":{"collapsed_sections":["yKepqgl6Klqk","1SXZryHsIfdo","imInxZeEK6kW","fdnOkeH0LM5P","7d3dY-JKL1bM","UPp-V4WZ-dtZ","n7tftfYWoiMN"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}